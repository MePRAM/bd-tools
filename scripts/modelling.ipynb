{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, RocCurveDisplay\n",
    "from sklearn.metrics import make_scorer, roc_auc_score, recall_score, accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and cleaning Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_file = \"\"\n",
    "output_location = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clean_dataset(csv_file, na_perc_limit, columns_to_delete=[]):\n",
    "    loaded_df = pd.read_csv(csv_file)\n",
    "    print(loaded_df.columns)\n",
    "    print(loaded_df.shape)\n",
    "    loaded_df = loaded_df.drop(columns=columns_to_delete)\n",
    "    tot = loaded_df.shape[0]\n",
    "    print(\"Cleaning dataset... \\n\")\n",
    "    for col in loaded_df.columns:\n",
    "        na_per = 1-len(loaded_df[col].dropna())/tot\n",
    "        if na_per > na_perc_limit:\n",
    "            print(f\"Column {col} --> %NaN = {na_per}. Removed\")\n",
    "            loaded_df = loaded_df.drop(columns=col)\n",
    "    print(\"\\n Dropping rows with NaNs...\")\n",
    "    loaded_df = loaded_df.dropna()\n",
    "    print(f\"\\nFinal columns: {loaded_df.columns}\")\n",
    "    print(loaded_df.shape)\n",
    "    return loaded_df\n",
    "\n",
    "columns_to_delete = [\"Unnamed: 0\", \"person_id\", \"fecha_ingreso_urgencias\", \"shock_septico\", \"foco\", \"sintoma_nan\", \"fecha_nacimiento\", \"codigo_postal\"]\n",
    "processed_df = load_clean_dataset(\n",
    "    csv_file = csv_file,\n",
    "    na_perc_limit = 0.25,\n",
    "    columns_to_delete = columns_to_delete\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in processed_df.columns:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training table size: {processed_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting data into train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = processed_df.drop(\"sepsis\", axis=1)\n",
    "y = processed_df[\"sepsis\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining models and hyperparameters for grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dict = {\n",
    "    \"logistic_regression\": {\n",
    "        \"model\": LogisticRegression(),\n",
    "        \"params\": {\n",
    "            \"solver\": [\"liblinear\", \"saga\"],\n",
    "            \"C\": [0.1, 1, 10, 100, 250, 500]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "randfor_dict = {\n",
    "    \"random_forest\": {\n",
    "        \"model\": RandomForestClassifier(),\n",
    "        \"params\": {\n",
    "            \"criterion\": [\"gini\", \"entropy\", \"log_loss\"],\n",
    "            \"n_estimators\": [10, 50, 100, 250, 500],\n",
    "            \"max_depth\": [None, 10, 20, 30],\n",
    "            \"min_samples_split\": [5, 10, 20, 30, 50]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "bayes_dict = {\n",
    "    \"bernoulli_bayes\": {\n",
    "        \"model\": BernoulliNB(),\n",
    "        \"params\": {\n",
    "            \"alpha\": [0.1, 0.5, 1.0, 1.5, 2.0],\n",
    "            \"fit_prior\": [True, False],\n",
    "            \"binarize\": [0.0, 0.5, 1.0, 1.5, 2.0]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "grad_boost_dict = {\n",
    "    \"gradient_boosting\": {\n",
    "        \"model\": GradientBoostingClassifier(),\n",
    "        \"params\": {\n",
    "            \"n_estimators\": [100, 200, 300],\n",
    "            \"learning_rate\": [0.01, 0.05, 0.1],\n",
    "            \"max_depth\": [3, 4, 5],\n",
    "            \"subsample\": [0.8, 0.9, 1.0],\n",
    "            \"min_samples_split\": [2, 5, 10]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\"\"\"svc_dict = {\n",
    "    \"svc\": {\n",
    "        \"model\": SVC(),\n",
    "        \"params\": {\n",
    "            \"C\": [0.1, 1, 10, 100],\n",
    "            \"kernel\": [\"linear\", \"rbf\", \"poly\", \"sigmoid\"],\n",
    "            \"gamma\": [\"scale\", \"auto\"]\n",
    "        }\n",
    "    }\n",
    "}\"\"\"\n",
    "grid_search_list = (log_dict, randfor_dict, bayes_dict, grad_boost_dict)\n",
    "all_grid_models = {k:v for model_dict in model_list for k,v in model_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import uniform, loguniform, randint\n",
    "\n",
    "log_dict = {\n",
    "    \"logistic_regression\": {\n",
    "        \"model\": LogisticRegression(),\n",
    "        \"params\": {\n",
    "            \"solver\": [\"liblinear\", \"saga\"],\n",
    "            \"C\": loguniform(0.1, 500)\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "randfor_dict = {\n",
    "    \"random_forest\": {\n",
    "        \"model\": RandomForestClassifier(),\n",
    "        \"params\": {\n",
    "            \"criterion\": [\"gini\", \"entropy\", \"log_loss\"],\n",
    "            \"n_estimators\": randint(10, 501),\n",
    "            \"max_depth\": [None, 10, 20, 30],\n",
    "            \"min_samples_split\": randint(5, 51)\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "bayes_dict = {\n",
    "    \"bernoulli_bayes\": {\n",
    "        \"model\": BernoulliNB(),\n",
    "        \"params\": {\n",
    "            \"alpha\": uniform(0.1, 2.0),\n",
    "            \"fit_prior\": [True, False],\n",
    "            \"binarize\": uniform(0.0, 2.0)\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "grad_boost_dict = {\n",
    "    \"gradient_boosting\": {\n",
    "        \"model\": GradientBoostingClassifier(),\n",
    "        \"params\": {\n",
    "            \"n_estimators\": randint(100, 301),\n",
    "            \"learning_rate\": loguniform(0.01, 0.1),\n",
    "            \"max_depth\": randint(3, 6),\n",
    "            \"subsample\": uniform(0.8, 0.2),\n",
    "            \"min_samples_split\": randint(2, 11)\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "random_search_list = (log_dict, randfor_dict, bayes_dict, grad_boost_dict)\n",
    "all_random_models = {k:v for model_dict in model_list for k,v in model_dict.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining search method arguments\n",
    "\n",
    "f1 score is very common for clinical data as it indicates a balance between precision and recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "random_search_args = {\n",
    "    \"search_model\" = RandomizedSearchCV()\n",
    "    \"search_params\": {\n",
    "        \"cv\":10, \"n_iter\":1000, \"n_jobs\":4, \"scoring\":{\n",
    "            \"f1\": \"f1\",\n",
    "            \"AUC\": \"roc_auc\",\n",
    "            \"Accuracy\": \"accuracy\",\n",
    "            \"Recall\": \"recall\"\n",
    "        },\n",
    "        \"refit\": \"f1\"\n",
    "    }\n",
    "}\n",
    "\n",
    "grid_search_args = {\n",
    "    \"search_model\" = GridSearchCV()\n",
    "    \"search_params\": {\n",
    "        \"cv\":5, \"scoring\":{\n",
    "            \"f1\": \"f1\",\n",
    "            \"AUC\": \"roc_auc\",\n",
    "            \"Accuracy\": \"accuracy\",\n",
    "            \"Recall\": \"recall\"\n",
    "        },\n",
    "        \"refit\": \"f1\"\n",
    "    }\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_search(models_params, search_args):\n",
    "    print(\"Given search args: \", search_args)\n",
    "    scores_list = []\n",
    "    for model_name, mp in models_params.items():\n",
    "        search_engine = search_args[\"search_model\"]\n",
    "        search_params = search_args[\"search_params\"]\n",
    "        cv_classifier = search_engine(mp[\"model\"], mp[\"params\"], **search_args)\n",
    "        cv_classifier.fit(X_train, y_train)\n",
    "        print(f\"{model_name} classifier results:\\n\", cv_classifier.best_score_, cv_classifier.best_params_)\n",
    "        scores_list.append({\n",
    "            \"model\": model_name,\n",
    "            \"best_score\": cv_classifier.best_score_,\n",
    "            \"best_params\": cv_classifier.best_params_,\n",
    "            \"cv_results\": cv_classifier.cv_results_\n",
    "        })\n",
    "    return scores_list\n",
    "\n",
    "def run_testing(processed_df, models_dict, search_args):\n",
    "    X = processed_df.drop(\"sepsis\", axis=1)\n",
    "    y = processed_df[\"sepsis\"]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=99)\n",
    "\n",
    "    scores = cv_search(models_params=models_dict, search_args=random_search_args, grid=False)\n",
    "\n",
    "    # Convert the results to a DataFrame\n",
    "    results_df = pd.DataFrame(scores, columns=[\"model\", \"best_score\", \"best_params\", \"cv_results\"])\n",
    "\n",
    "    results_df.to_csv(os.path.join(output_location, \"results_df.csv\"))\n",
    "    pd.DataFrame(scores[\"cv_results\"]).to_csv(os.path.join(output_location, \"cv_results.csv\"))\n",
    "\n",
    "    # Display the results\n",
    "    print(results_df)\n",
    "\n",
    "    # Select the best model and evaluate it on the test set\n",
    "    best_model_name = results_df.loc[results_df[\"best_score\"].idxmax()][\"model\"]\n",
    "    best_model_params = results_df.loc[results_df[\"best_score\"].idxmax()][\"best_params\"]\n",
    "\n",
    "    best_model = models_dict[best_model_name][\"model\"]\n",
    "    best_model.set_params(**best_model_params)\n",
    "    best_model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    y_pred_prob = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    print(f\"Found best model: {best_model_name} with parameters: {best_model_params}\")\n",
    "    print(\"Scores table: \\n\", classification_report(y_test, y_pred))\n",
    "    \n",
    "    try:\n",
    "        importance_dict = {y:x for x,y in zip(X_train.columns, best_model.feature_log_prob_[0])}\n",
    "    except Exception:\n",
    "        try:\n",
    "            importance_dict = {y:x for x,y in zip(X_train.columns, best_model.feature_importances_)}\n",
    "        except Exception:\n",
    "            importance_dict = {}\n",
    "    if importance_dict:\n",
    "        importance_df = pd.DataFrame({x:importance_dict[x] for x in sorted(importance_dict, reverse=True)}, columns=[\"Column\", \"Weight\"])\n",
    "        importance_df.to_csv(os.path.join(output_location, f\"{best_model_name}_importance_df.csv\"))\n",
    "    else:\n",
    "        print(f\"Could not extract importance values for best model {best_model_name}:{best_model_params}\")\n",
    "    class_report_df = pd.DataFrame(classification_report(y_test, y_pred, output_dict=True)).transpose()\n",
    "    class_report_df[\"roc_auc\"] = roc_auc_score(y_test, y_pred_prob)\n",
    "    class_report_df.to_csv(os.path.join(output_location, \"class_report.csv\"))\n",
    "    return results_df, class_report_df\n",
    "\n",
    "models_dict = randfor_dict\n",
    "\n",
    "results_df, class_report_df = run_testing(processed_df, models_dict, search_args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cv_results_df = pd.DataFrame(results_df[0][\"cv_results\"])\n",
    "fig, ax = plt.subplots(figsize=(12,6))\n",
    "for col in [\"mean_test_AUC\", \"mean_test_f1\", \"mean_test_Accuracy\", \"mean_test_Recall\"]:\n",
    "    sns.kdeplot(data=cv_results_df, x=col, label=col)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(best_model.feature_log_prob_[0]), len(X_train.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
