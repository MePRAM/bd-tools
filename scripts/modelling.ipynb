{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, RocCurveDisplay\n",
    "from sklearn.metrics import make_scorer, roc_auc_score, recall_score, accuracy_score, confusion_matrix\n",
    "from sklearn.inspection import permutation_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and cleaning Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_file  =\n",
    "output_location ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clean_dataset(loaded_df, na_perc_limit, columns_to_delete=[]):\n",
    "    print(loaded_df.shape)\n",
    "    loaded_df = loaded_df.drop(columns=columns_to_delete)\n",
    "    tot = loaded_df.shape[0]\n",
    "    print(\"Cleaning dataset... \\n\")\n",
    "    for col in loaded_df.columns:\n",
    "        na_per = 1-len(loaded_df[col].dropna())/tot\n",
    "        if na_per > na_perc_limit:\n",
    "            print(f\"Column {col} --> %NaN = {na_per}. Removed\")\n",
    "            loaded_df = loaded_df.drop(columns=col)\n",
    "    print(\"\\n Dropping rows with NaNs...\")\n",
    "    loaded_df = loaded_df.dropna()\n",
    "    print(f\"\\nFinal columns: {loaded_df.columns}\")\n",
    "    print(loaded_df.shape)\n",
    "    return loaded_df\n",
    "\n",
    "columns_to_delete = [\"Unnamed: 0\", \"person_id\", \"fecha_ingreso_urgencias\", \"shock_septico\", \"foco\", \"sintoma_nan\", \"fecha_nacimiento\", \"codigo_postal\"]\n",
    "\n",
    "def combine_columns(df_to_clean, column_list, new_col_name):\n",
    "    df_to_clean[new_col_name] = df_to_clean[column_list].sum(axis=1)\n",
    "    clean_df = df_to_clean.drop(columns=column_list)\n",
    "    return clean_df\n",
    "\n",
    "df_to_clean = pd.read_csv(database_file)\n",
    "hepatic_cols = [c for c in df_to_clean.columns if \"hepatopatia\" in c]\n",
    "tumor_cols = [c for c in df_to_clean.columns if \"cancer\" in c]\n",
    "for new_name, col_list in {\"enf_hepaticas\": hepatic_cols, \"tumores\": tumor_cols}.items():\n",
    "    df_to_clean = combine_columns(df_to_clean, col_list, new_name)\n",
    "\n",
    "processed_df = load_clean_dataset(\n",
    "    loaded_df = df_to_clean,\n",
    "    na_perc_limit = 0.1,\n",
    "    columns_to_delete = columns_to_delete\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting data into train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = processed_df.drop(\"sepsis\", axis=1)\n",
    "y = processed_df[\"sepsis\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining models and hyperparameters for grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dict = {\n",
    "    \"logistic_regression\": {\n",
    "        \"model\": LogisticRegression(),\n",
    "        \"params\": {\n",
    "            \"solver\": [\"lbfgs\", \"liblinear\", \"saga\"],\n",
    "            \"C\": [0.1, 1, 10, 100, 250, 500],\n",
    "            \"penalty\": [\"l2\", \"l1\", \"elasticnet\"],\n",
    "            \"max_iter\": [100, 1000, 5000]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "randfor_dict = {\n",
    "    \"random_forest\": {\n",
    "        \"model\": RandomForestClassifier(),\n",
    "        \"params\": {\n",
    "            \"criterion\": [\"gini\", \"entropy\", \"log_loss\"],\n",
    "            \"n_estimators\": [10, 50, 100, 250, 500],\n",
    "            \"max_depth\": [None, 10, 20, 30],\n",
    "            \"min_samples_split\": [5, 10, 20, 30, 50]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "bayes_dict = {\n",
    "    \"bernoulli_bayes\": {\n",
    "        \"model\": BernoulliNB(),\n",
    "        \"params\": {\n",
    "            \"alpha\": [0.1, 0.5, 1.0, 1.5, 2.0],\n",
    "            \"fit_prior\": [True, False],\n",
    "            \"binarize\": [0.0, 0.5, 1.0, 1.5, 2.0]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "svc_dict = {\n",
    "    \"svc\": {\n",
    "        \"model\": SVC(),\n",
    "        \"params\": {\n",
    "            \"C\": [0.1, 1, 10, 100],\n",
    "            \"kernel\": [\"linear\", \"rbf\", \"poly\", \"sigmoid\"],\n",
    "            \"gamma\": [\"scale\", \"auto\"]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "grid_search_list = (log_dict, randfor_dict, bayes_dict, grad_boost_dict)\n",
    "all_grid_models = {k:v for model_dict in model_list for k,v in model_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import uniform, loguniform, randint\n",
    "\n",
    "log_dict = {\n",
    "    \"logistic_regression\": {\n",
    "        \"model\": LogisticRegression(),\n",
    "        \"params\": {\n",
    "            \"solver\": [\"liblinear\", \"saga\"],\n",
    "            \"C\": loguniform(0.1, 500),\n",
    "            \"penalty\": [\"l2\", \"l1\", \"elasticnet\"],\n",
    "            \"max_iter\": [100, 1000, 5000]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "randfor_dict = {\n",
    "    \"random_forest\": {\n",
    "        \"model\": RandomForestClassifier(),\n",
    "        \"params\": {\n",
    "            \"criterion\": [\"gini\", \"entropy\", \"log_loss\"],\n",
    "            \"n_estimators\": randint(10, 501),\n",
    "            \"max_depth\": [None, 10, 20, 30],\n",
    "            \"min_samples_split\": randint(5, 51)\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "bayes_dict = {\n",
    "    \"bernoulli_bayes\": {\n",
    "        \"model\": BernoulliNB(),\n",
    "        \"params\": {\n",
    "            \"alpha\": uniform(0.1, 2.0),\n",
    "            \"fit_prior\": [True, False],\n",
    "            \"binarize\": uniform(0.0, 2.0)\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "random_search_list = (log_dict, randfor_dict, bayes_dict, grad_boost_dict)\n",
    "all_random_models = {k:v for model_dict in model_list for k,v in model_dict.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining search method arguments\n",
    "\n",
    "f1 score is very common for clinical data as it indicates a balance between precision and recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "random_search_args = {\n",
    "    \"search_model\": RandomizedSearchCV,\n",
    "    \"search_params\": {\n",
    "        \"cv\":10, \"n_iter\":1000, \"n_jobs\":4, \"scoring\":{\n",
    "            \"f1\": \"f1\",\n",
    "            \"AUC\": \"roc_auc\",\n",
    "            \"Accuracy\": \"accuracy\",\n",
    "            \"Recall\": \"recall\"\n",
    "        },\n",
    "        \"refit\": \"f1\"\n",
    "    }\n",
    "}\n",
    "\n",
    "grid_search_args = {\n",
    "    \"search_model\": GridSearchCV,\n",
    "    \"search_params\": {\n",
    "        \"cv\":5, \"scoring\":{\n",
    "            \"f1\": \"f1\",\n",
    "            \"AUC\": \"roc_auc\",\n",
    "            \"Accuracy\": \"accuracy\",\n",
    "            \"Recall\": \"recall\"\n",
    "        },\n",
    "        \"refit\": \"f1\"\n",
    "    }\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_search(models_params, search_args):\n",
    "    print(\"Given search args: \", search_args)\n",
    "    scores_list = []\n",
    "    search_engine = search_args[\"search_model\"]\n",
    "    search_params = search_args[\"search_params\"]\n",
    "    for model_name, mp in models_params.items():\n",
    "        print(f\"Starting cross-validation for {model_name}...\")\n",
    "        cv_classifier = search_engine(mp[\"model\"], mp[\"params\"], **search_params)\n",
    "        cv_classifier.fit(X_train, y_train)\n",
    "        print(f\"Results for {model_name}:\\n\", cv_classifier.best_score_, cv_classifier.best_params_)\n",
    "        scores_list.append({\n",
    "            \"model\": model_name,\n",
    "            \"best_score\": cv_classifier.best_score_,\n",
    "            \"best_params\": cv_classifier.best_params_,\n",
    "            \"cv_results\": cv_classifier.cv_results_\n",
    "        })\n",
    "    return scores_list\n",
    "\n",
    "def get_feature_weights(best_model, X, y, n_repeats, state, score):\n",
    "    return permutation_importance(best_model, X, y, n_repeats=n_repeats, random_state=state, scoring=score)\n",
    "\n",
    "def run_testing(X, y, models_dict, search_args):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=99)\n",
    "\n",
    "    scores = cv_search(models_params=models_dict, search_args=search_args)\n",
    "\n",
    "    print(\"Training finished\")\n",
    "\n",
    "    # Convert the results to a DataFrame\n",
    "    results_df = pd.DataFrame(scores, columns=[\"model\", \"best_score\", \"best_params\", \"cv_results\"])\n",
    "    results_df.to_csv(os.path.join(output_location, \"results_df.csv\"))\n",
    "\n",
    "    # Select the best model and evaluate it on the test set\n",
    "    best_model_name = results_df.loc[results_df[\"best_score\"].idxmax()][\"model\"]\n",
    "    best_model_params = results_df.loc[results_df[\"best_score\"].idxmax()][\"best_params\"]\n",
    "\n",
    "    best_model = models_dict[best_model_name][\"model\"]\n",
    "    best_model.set_params(**best_model_params)\n",
    "    best_model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    y_pred_prob = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    print(f\"Found best model: {best_model_name} with parameters: {best_model_params}\")\n",
    "    print(\"Scores table: \\n\", classification_report(y_test, y_pred))\n",
    "    try:\n",
    "        feature_weights = get_feature_weights(best_model, X_train, y_train, n_repeats=10, state=99, score=\"f1\")\n",
    "    except Exception:\n",
    "        feature_weights = {}\n",
    "    if feature_weights:\n",
    "        importance_df = pd.DataFrame({y:x for x,y in sorted(zip(feature_weights[\"importances_mean\"], X_train.columns), reverse=True)}, index=[0])\n",
    "        importance_df.to_csv(os.path.join(output_location, f\"{best_model_name}_importance_df.csv\"))\n",
    "    else:\n",
    "        print(f\"Could not extract importance values for best model {best_model_name}:{best_model_params}\")\n",
    "    class_report_df = pd.DataFrame(classification_report(y_test, y_pred, output_dict=True)).transpose()\n",
    "    class_report_df[\"roc_auc\"] = roc_auc_score(y_test, y_pred_prob)\n",
    "    class_report_df.to_csv(os.path.join(output_location, f\"{best_model_name}_class_report.csv\"))\n",
    "    return results_df, class_report_df\n",
    "\n",
    "models_dict = all_random_models\n",
    "\n",
    "search_args = random_search_args\n",
    "X = processed_df.drop(\"sepsis\", axis=1)\n",
    "y = processed_df[\"sepsis\"]\n",
    "results_df, class_report_df = run_testing(X, y, models_dict, search_args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot scores distribution along cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_cv_results(cv_results_df):\n",
    "    fig, ax = plt.subplots(figsize=(12,6))\n",
    "    for col in [\"mean_test_AUC\", \"mean_test_f1\", \"mean_test_Accuracy\", \"mean_test_Recall\"]:\n",
    "        sns.kdeplot(data=cv_results_df, x=col, label=col)\n",
    "    ax.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "for idx, row in results_df.iterrows():\n",
    "    cv_results = pd.DataFrame(row[\"cv_results\"])\n",
    "    plot_cv_results(cv_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boost and other approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDCLassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, BaggingClassifier\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_boost_dict = {\n",
    "    \"gradient_boosting\": {\n",
    "        \"model\": GradientBoostingClassifier(),\n",
    "        \"params\": {\n",
    "            \"n_estimators\": randint(100, 500),\n",
    "            \"learning_rate\": loguniform(0.001, 0.1),\n",
    "            \"max_depth\": randint(3, 10),\n",
    "            \"subsample\": uniform(0.7, 1),\n",
    "            \"min_samples_split\": randint(5, 20)\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "sgdc_dict = {\n",
    "    \"sgdc_classifier\": {\n",
    "        \"model\": SGDCLassifier(),\n",
    "        \"params\": {\n",
    "            \"loss\": [\"hinge\", \"log_loss\", \"modified_huber\", \"squared_hinge\", \"perceptron\"],\n",
    "            \"alpha\": loguniform(0.0001, 0.1),\n",
    "            \"max_iter\": randint(1000, 20000),\n",
    "            \"early_stopping\": [True]\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
