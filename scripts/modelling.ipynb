{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import classification_report, RocCurveDisplay\n",
    "from sklearn.metrics import make_scorer, roc_auc_score, recall_score, accuracy_score, confusion_matrix\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and cleaning Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_file  = \"\"\n",
    "output_location =  \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clean_dataset(loaded_df, na_perc_limit, columns_to_delete=[]):\n",
    "    print(loaded_df.shape)\n",
    "    loaded_df = loaded_df.drop(columns=columns_to_delete)\n",
    "    tot = loaded_df.shape[0]\n",
    "    print(\"Cleaning dataset... \\n\")\n",
    "    for col in loaded_df.columns:\n",
    "        na_per = 1-len(loaded_df[col].dropna())/tot\n",
    "        if na_per > na_perc_limit:\n",
    "            print(f\"Column {col} --> %NaN = {na_per}. Removed\")\n",
    "            loaded_df = loaded_df.drop(columns=col)\n",
    "    print(\"\\n Dropping rows with NaNs...\")\n",
    "    loaded_df = loaded_df.dropna()\n",
    "    print(f\"\\nFinal columns: {loaded_df.columns}\")\n",
    "    print(loaded_df.shape)\n",
    "    return loaded_df\n",
    "\n",
    "columns_to_delete = [\"Unnamed: 0\", \"person_id\", \"fecha_ingreso_urgencias\", \"shock_septico\", \"foco\", \"sintoma_nan\", \"fecha_nacimiento\", \"codigo_postal\", \"center\", \"dag\"]\n",
    "\n",
    "def combine_columns(df_to_clean, column_list, new_col_name):\n",
    "    df_to_clean[new_col_name] = df_to_clean[column_list].sum(axis=1).astype(int)\n",
    "    clean_df = df_to_clean.drop(columns=column_list)\n",
    "    return clean_df\n",
    "\n",
    "df_to_clean = pd.read_csv(database_file)\n",
    "hepatic_cols = [c for c in df_to_clean.columns if \"hepatopatia\" in c]\n",
    "tumor_cols = [c for c in df_to_clean.columns if \"cancer\" in c]\n",
    "for new_name, col_list in {\"enf_hepaticas\": hepatic_cols, \"tumores\": tumor_cols}.items():\n",
    "    df_to_clean = combine_columns(df_to_clean, col_list, new_name)\n",
    "\n",
    "processed_df = load_clean_dataset(\n",
    "    loaded_df = df_to_clean,\n",
    "    na_perc_limit = 0.1,\n",
    "    columns_to_delete = columns_to_delete\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting data into train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"X = processed_df.drop(\"sepsis\", axis=1)\n",
    "y = processed_df[\"sepsis\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining models and hyperparameters for grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dict = {\n",
    "    \"logistic_regression\": {\n",
    "        \"model\": LogisticRegression(),\n",
    "        \"params\": {\n",
    "            \"solver\": [\"lbfgs\", \"liblinear\", \"saga\"],\n",
    "            \"C\": [0.1, 1, 10, 100, 250, 500],\n",
    "            \"penalty\": [\"l2\", \"l1\", \"elasticnet\"],\n",
    "            \"max_iter\": [100, 1000, 5000]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "randfor_dict = {\n",
    "    \"random_forest\": {\n",
    "        \"model\": RandomForestClassifier(),\n",
    "        \"params\": {\n",
    "            \"criterion\": [\"gini\", \"entropy\", \"log_loss\"],\n",
    "            \"n_estimators\": [10, 50, 100, 250, 500],\n",
    "            \"max_depth\": [None, 10, 20, 30],\n",
    "            \"min_samples_split\": [5, 10, 20, 30, 50]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "bayes_dict = {\n",
    "    \"bernoulli_bayes\": {\n",
    "        \"model\": BernoulliNB(),\n",
    "        \"params\": {\n",
    "            \"alpha\": [0.1, 0.5, 1.0, 1.5, 2.0],\n",
    "            \"fit_prior\": [True, False],\n",
    "            \"binarize\": [0.0, 0.5, 1.0, 1.5, 2.0]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "svc_dict = {\n",
    "    \"svc\": {\n",
    "        \"model\": SVC(),\n",
    "        \"params\": {\n",
    "            \"C\": [0.1, 1, 10, 100],\n",
    "            \"kernel\": [\"linear\", \"rbf\", \"poly\", \"sigmoid\"],\n",
    "            \"gamma\": [\"scale\", \"auto\"]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\"\"\"grid_search_list = (log_dict, randfor_dict, bayes_dict, grad_boost_dict)\n",
    "all_grid_models = {k:v for model_dict in model_list for k,v in model_dict.items()}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models that require normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import uniform, loguniform, randint\n",
    "\n",
    "log_dict = {\n",
    "    \"logistic_regression\": {\n",
    "        \"model\": LogisticRegression(),\n",
    "        \"grid_params\": {\n",
    "            \"solver\": [\"lbfgs\", \"liblinear\", \"saga\"],\n",
    "            \"C\": [0.1, 1, 10, 100, 250, 500],\n",
    "            \"penalty\": [\"l2\", \"l1\", \"elasticnet\"],\n",
    "            \"max_iter\": [100, 1000, 5000]\n",
    "        },\n",
    "        \"random_params\": {\n",
    "            \"solver\": [\"liblinear\", \"saga\"],\n",
    "            \"C\": loguniform(0.1, 500),\n",
    "            \"penalty\": [\"l2\", \"l1\", \"elasticnet\"],\n",
    "            \"max_iter\": [100, 1000, 5000]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "svc_dict = {\n",
    "    \"svc\": {\n",
    "        \"model\": SVC(),\n",
    "        \"grid_params\": {\n",
    "            \"C\": [0.1, 1, 10, 100],\n",
    "            \"kernel\": [\"linear\", \"rbf\", \"poly\", \"sigmoid\"],\n",
    "            \"gamma\": [\"scale\", \"auto\"]\n",
    "        },\n",
    "        \"random_params\": {\n",
    "            \"C\": np.logspace(-3, 3, 7),\n",
    "            \"kernel\": [\"linear\", \"rbf\", \"poly\", \"sigmoid\"],\n",
    "            \"degree\": [2, 3, 4, 5],          # Only used for \"poly\" kernel\n",
    "            \"gamma\": [\"scale\", \"auto\", 0.001, 0.01, 0.1, 1, 10], # Only used for \"rbf\", \"poly\", and \"sigmoid\" kernels\n",
    "            \"coef0\": [0.0, 0.1, 0.5, 1.0],  # Only used for \"poly\" and \"sigmoid\" kernels\n",
    "            \"class_weight\": [None, \"balanced\"],\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "sgdc_dict = {\n",
    "    \"sgdc_classifier\": {\n",
    "        \"model\": SGDClassifier(),\n",
    "        \"grid_params\": {\n",
    "            \"loss\": [\"hinge\", \"log_loss\", \"modified_huber\", \"squared_hinge\", \"perceptron\"],\n",
    "            \"alpha\": [0.0001, 0.001, 0.01],\n",
    "            \"max_iter\": [100, 1000, 5000],\n",
    "            \"early_stopping\": [True]\n",
    "        },\n",
    "        \"random_params\": {\n",
    "            \"loss\": [\"hinge\", \"log_loss\", \"modified_huber\", \"squared_hinge\", \"perceptron\"],\n",
    "            \"alpha\": loguniform(0.0001, 0.1),\n",
    "            \"max_iter\": randint(100, 10000),\n",
    "            \"early_stopping\": [True]\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "knn_dict = {\n",
    "    \"knn_classifier\": {\n",
    "        \"model\": KNeighborsClassifier(),\n",
    "        \"grid_params\": {\n",
    "            \"n_neighbors\": list(range(1, 31)),\n",
    "            \"weights\": [\"uniform\", \"distance\"],\n",
    "            \"algorithm\": [\"auto\", \"ball_tree\", \"kd_tree\"],\n",
    "            \"leaf_size\": list(range(10, 101, 5)),\n",
    "            \"metric\": [\"euclidean\", \"manhattan\"],\n",
    "        },\n",
    "        \"random_params\": {\n",
    "            \"n_neighbors\": list(range(1, 31)),\n",
    "            \"weights\": [\"uniform\", \"distance\"],\n",
    "            \"algorithm\": [\"auto\", \"ball_tree\", \"kd_tree\"],\n",
    "            \"leaf_size\": list(range(10, 101, 5)),\n",
    "            \"metric\": [\"euclidean\", \"manhattan\"],\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "scaled_models_list = (log_dict, svc_dict, sgdc_dict, knn_dict)\n",
    "random_scaled_models = {k:v for model_dict in scaled_models_list for k,v in model_dict.items() if k != \"grid_params\"}\n",
    "grid_scaled_models = {k:v for model_dict in scaled_models_list for k,v in model_dict.items() if k != \"random_params\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randfor_dict = {\n",
    "    \"random_forest\": {\n",
    "        \"model\": RandomForestClassifier(),\n",
    "        \"params\": {\n",
    "            \"criterion\": [\"gini\", \"entropy\", \"log_loss\"],\n",
    "            \"n_estimators\": randint(10, 501),\n",
    "            \"max_depth\": [None, 10, 20, 30],\n",
    "            \"min_samples_split\": randint(5, 51)\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "bayes_dict = {\n",
    "    \"bernoulli_bayes\": {\n",
    "        \"model\": BernoulliNB(),\n",
    "        \"params\": {\n",
    "            \"alpha\": uniform(0.1, 2.0),\n",
    "            \"fit_prior\": [True, False],\n",
    "            \"binarize\": uniform(0.0, 2.0)\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "grad_boost_dict = {\n",
    "    \"gradient_boosting\": {\n",
    "        \"model\": GradientBoostingClassifier(),\n",
    "        \"params\": {\n",
    "            \"n_estimators\": randint(100, 500),\n",
    "            \"learning_rate\": loguniform(0.001, 0.1),\n",
    "            \"max_depth\": randint(3, 10),\n",
    "            \"subsample\": uniform(0.7, 1),\n",
    "            \"min_samples_split\": randint(5, 20)\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "\"\"\"random_search_list = (log_dict, randfor_dict, bayes_dict, grad_boost_dict)\n",
    "all_random_models = {k:v for model_dict in model_list for k,v in model_dict.items()}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining search method arguments\n",
    "\n",
    "f1 score is very common for clinical data as it indicates a balance between precision and recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "random_search_args = {\n",
    "    \"search_model\": RandomizedSearchCV,\n",
    "    \"search_params\": {\n",
    "        \"cv\":10, \"n_iter\":1000, \"n_jobs\":4, \"scoring\":{\n",
    "            \"f1\": \"f1\",\n",
    "            \"AUC\": \"roc_auc\",\n",
    "            \"Accuracy\": \"accuracy\",\n",
    "            \"Recall\": \"recall\"\n",
    "        },\n",
    "        \"refit\": \"f1\"\n",
    "    }\n",
    "}\n",
    "\n",
    "grid_search_args = {\n",
    "    \"search_model\": GridSearchCV,\n",
    "    \"search_params\": {\n",
    "        \"cv\":10, \"scoring\": \"accuracy\"\n",
    "    }\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_search(models_params, search_args, X_train, y_train):\n",
    "    print(\"Given search args: \", search_args)\n",
    "    scores_list = []\n",
    "    search_engine = search_args[\"search_model\"]\n",
    "    search_params = search_args[\"search_params\"]\n",
    "    for model_name, mp in models_params.items():\n",
    "        print(f\"Starting cross-validation for {model_name}...\")\n",
    "        cv_classifier = search_engine(mp[\"model\"], mp[\"random_params\"], **search_params)\n",
    "        start_time = time.time()    \n",
    "        cv_classifier.fit(X_train, y_train)\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "\n",
    "        print(f\"Results for {model_name}:\\n\", cv_classifier.best_score_, cv_classifier.best_params_)\n",
    "        scores_list.append({\n",
    "            \"model\": model_name,\n",
    "            \"best_score\": cv_classifier.best_score_,\n",
    "            \"best_params\": cv_classifier.best_params_,\n",
    "            \"elapsed_time\": elapsed_time,\n",
    "            \"cv_results\": pd.DataFrame(cv_classifier.cv_results_)\n",
    "        })\n",
    "    return scores_list\n",
    "\n",
    "def get_feature_weights(best_model, X, y, n_repeats, state, score):\n",
    "    return permutation_importance(best_model, X, y, n_repeats=n_repeats, random_state=state, scoring=score)\n",
    "\n",
    "def fit_test_model(X_train, X_test, y_train, y_test, features, best_model, best_model_params):\n",
    "    best_model.set_params(**best_model_params)\n",
    "    best_model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    y_pred_prob = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    print(f\"Found best model: {type(best_model).__name__} with parameters: {best_model_params}\")\n",
    "    try:\n",
    "        feature_weights = get_feature_weights(best_model, X_train, y_train, n_repeats=10, state=99, score=\"f1\")\n",
    "    except Exception:\n",
    "        feature_weights = {}\n",
    "    if feature_weights:\n",
    "        importance_df = pd.DataFrame({y:x for x,y in sorted(zip(feature_weights[\"importances_mean\"], features), reverse=True)}, index=[0])\n",
    "        importance_df.to_csv(os.path.join(output_location, f\"{type(best_model).__name__}_importance_df.csv\"))\n",
    "    else:\n",
    "        print(f\"Could not extract importance values for best model {type(best_model).__name__}:{best_model_params}\")\n",
    "    class_report_df = pd.DataFrame(classification_report(y_test, y_pred, output_dict=True)).transpose()\n",
    "    class_report_df[\"roc_auc\"] = roc_auc_score(y_test, y_pred_prob)\n",
    "    print(\"Scores table: \\n\", class_report_df)\n",
    "    class_report_df.to_csv(os.path.join(output_location, f\"{type(best_model).__name__}_class_report.csv\"))\n",
    "    return class_report_df\n",
    "\n",
    "def run_testing(X, y, models_dict, search_args, scaler=None):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=99)\n",
    "    features = X_train.columns\n",
    "    if scaler is not None:\n",
    "        try:\n",
    "            X_train = pd.DataFrame(scaler.fit_transform(X_train), columns=features)\n",
    "            X_test = scaler.fit_transform(X_test)\n",
    "            print(f\"Scaled data with given scaler {scaler}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not scale data with provided scaler {scaler}: {e}\")\n",
    "            return\n",
    "\n",
    "    scores = cv_search(models_params=models_dict, search_args=search_args, X_train=X_train, y_train=y_train)\n",
    "\n",
    "    print(\"Training finished\")\n",
    "\n",
    "    # Convert the results to a DataFrame\n",
    "    results_df = pd.DataFrame(scores, columns=[\"model\", \"best_score\", \"best_params\", \"elapsed_time\"])\n",
    "    results_df.to_csv(os.path.join(output_location, \"results_df.csv\"))\n",
    "    for row in scores:\n",
    "        model_name = row[\"model\"]\n",
    "        row[\"cv_results\"].to_csv(os.path.join(output_location, f\"{model_name}_cv_results.csv\"))\n",
    "\n",
    "    # Select the best model and evaluate it on the test set\n",
    "    best_model_name = results_df.loc[results_df[\"best_score\"].idxmax()][\"model\"]\n",
    "    best_model_params = results_df.loc[results_df[\"best_score\"].idxmax()][\"best_params\"]\n",
    "\n",
    "    best_model = models_dict[best_model_name][\"model\"]\n",
    "    class_report_df = fit_test_model(X_train, X_test, y_train, y_test, features, best_model, best_model_params)\n",
    "    return results_df, class_report_df\n",
    "\n",
    "search_args = random_search_args\n",
    "output_location = \"/home/pmata/mepram_dataframes/test8_results/\"\n",
    "models_dict = {\"LogisticRegression\": random_scaled_models[\"logistic_regression\"]}\n",
    "\n",
    "X = processed_df.drop(\"sepsis\", axis=1)\n",
    "y = processed_df[\"sepsis\"]\n",
    "\n",
    "results_df, class_report_df = run_testing(\n",
    "    X=X, y=y, models_dict=models_dict, search_args=search_args, scaler=MinMaxScaler()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot scores distribution along cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_cv_results(cv_results_df, title):\n",
    "    fig, ax = plt.subplots(figsize=(12,6))\n",
    "    for col in [\"mean_test_AUC\", \"mean_test_f1\", \"mean_test_Accuracy\", \"mean_test_Recall\"]:\n",
    "        sns.kdeplot(data=cv_results_df, x=col, label=col)\n",
    "    ax.legend()\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "for idx, row in results_df.iterrows():\n",
    "    cv_results = pd.DataFrame(row[\"cv_results\"])\n",
    "    plot_cv_results(cv_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boost and other approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from xgboost import cv as xgboost_cv\n",
    "from xgboost import DMatrix\n",
    "import optuna\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_dict = {\n",
    "    \"XGBoost_classifier\": {\n",
    "        \"model\": XGBClassifier(),\n",
    "        \"random_params\": {\n",
    "            \"objective\": [\"binary:logistic\"],\n",
    "            'max_depth': stats.randint(3, 10),\n",
    "            'learning_rate': stats.uniform(0.01, 0.1),\n",
    "            'subsample': stats.uniform(0.5, 0.5),\n",
    "            'n_estimators':stats.randint(50, 200)\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "search_args = random_search_args\n",
    "output_location = \"/home/pmata/mepram_dataframes/test_xgboost_results/\"\n",
    "param_grid = {\n",
    "    'reg_alpha':[1e-5, 1e-2, 0.1, 100],\n",
    "    \"learning_rate\": [0.001, 0.1, 0.5],\n",
    "    \"n_estimators\": [3,5, 10, 15],\n",
    "    \"max_depth\": 5,\n",
    "    \"min_child_weight\": 2,\n",
    "    \"gamma\": 0.1,\n",
    "    subsample=0.85,\n",
    "    colsample_bytree=0.8,\n",
    "    objective= 'binary:logistic',\n",
    "    nthread=4,\n",
    "    scale_pos_weight=1,\n",
    "    seed=27\n",
    "} \n",
    "              \n",
    "grid_search = GridSearchCV(estimator = XGBClassifier(),\n",
    "    param_grid = param_grid,\n",
    "    scoring='f1',\n",
    "    n_jobs=4,\n",
    "    cv=10,\n",
    "    verbose=10\n",
    ") \n",
    "\n",
    "X = processed_df.drop(\"sepsis\", axis=1)\n",
    "y = processed_df[\"sepsis\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=99)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "print('Best Grid Search Parameters :',grid_search.best_params_)\n",
    "print('Best Grid Search Score : ',grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import xgboost as xgb\n",
    "import joblib\n",
    "from sklearn.metrics import f1_score, log_loss\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=99)\n",
    "\n",
    "def objective(trial):\n",
    "    # Define the hyperparameter search space\n",
    "    param = {\n",
    "        'verbosity': 0,\n",
    "        'objective': 'binary:logistic',  # Binary classification\n",
    "        'eval_metric': 'auc',\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 400),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'gamma': trial.suggest_float('gamma', 0, 0.5),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0.01, 10.0, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0.01, 10.0, log=True),\n",
    "        'scale_pos_weight': trial.suggest_int('scale_pos_weight', 1, 10)\n",
    "    }\n",
    "    \n",
    "    # Train the model\n",
    "    model = xgb.XGBClassifier(**param)\n",
    "    model.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=False)\n",
    "    \n",
    "    # Predict and calculate accuracy\n",
    "    y_pred = model.predict(X_test)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    logloss = log_loss(y_test, y_pred)\n",
    "    return f1, logloss\n",
    "\n",
    "# Create a study object and optimize the objective function\n",
    "study = optuna.create_study(directions=[\"maximize\", \"minimize\"])\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best parameters: \", study.best_params)\n",
    "print(\"Best score: \", study.best_value)\n",
    "\n",
    "joblib.dump(study, os.path.join(output_location, \"xgboost_study.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "processed_df[\"enf_hepaticas\"] = processed_df[\"enf_hepaticas\"].astype(int)\n",
    "X = processed_df.drop(\"sepsis\", axis=1)\n",
    "y = processed_df[\"sepsis\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=99)\n",
    "some_params = {'n_estimators': 252, 'max_depth': 13, 'learning_rate': 0.029417993022890093, 'subsample': 0.7878338148028812, 'colsample_bytree': 0.7735167084031885, 'gamma': 0.034664742223686935, 'min_child_weight': 2, 'reg_alpha': 0.11711315383329728, 'reg_lambda': 0.09837634948322023, 'scale_pos_weight': 2}\n",
    "xxx = xgb.XGBClassifier(**some_params).fit(X_train, y_train, verbose=False)\n",
    "y_pred = xxx.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "study = joblib.load(os.path.join(output_location,\"xgboost_study.pkl\"))\n",
    "print(study.best_params, study.best_value)\n",
    "\n",
    "X = processed_df.drop(\"sepsis\", axis=1)\n",
    "y = processed_df[\"sepsis\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=99)\n",
    "classif = xgb.XGBClassifier(**study.best_params).fit(X_train, y_train, verbose=False)\n",
    "y_pred = classif.predict(X_test)\n",
    "y_pred_prob = classif.predict_proba(X_test)[:, 1]\n",
    "roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
    "print(f\"Best model {type(classif)} with roc_auc = {roc_auc} and best params: {study.best_params}\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna.visualization as vis\n",
    "\n",
    "vis.plot_optimization_history(study)\n",
    "vis.plot_param_importances(study)\n",
    "vis.plot_slice(study)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
