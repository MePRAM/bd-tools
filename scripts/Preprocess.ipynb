{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "from ydata_profiling import ProfileReport\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_location = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def codes_to_names(tab_to_process, codes_tab, shortname):\n",
    "    errors = []\n",
    "    codes_group = codes_tab.filter(pl.col(\"form\") == shortname)\n",
    "    try:\n",
    "        codes_group = codes_group.with_columns(pl.col(\"value\").cast(pl.Int64))\n",
    "    except Exception:\n",
    "        pass\n",
    "    codes_group = codes_group.filter(pl.col(\"value\").is_not_null())\n",
    "    if codes_group.height == 0:\n",
    "        print(\"NO VARS TO RENAME IN:\", shortname)\n",
    "        return tab_to_process, errors\n",
    "    for varname, vartab in codes_group.group_by(\"variable\"):\n",
    "        if varname[0] == \"fenotipo_resist_colo\":\n",
    "            varname = [\"feno_resist_colo\"]\n",
    "        column_dict = vartab.select([\"value\", \"name\"]).to_dict(as_series=False)\n",
    "        codes2names_dict = {\n",
    "            item[0]: item[1] for item in zip(column_dict[\"value\"], column_dict[\"name\"])\n",
    "        }\n",
    "        column = varname[0]\n",
    "        print(f\"RENAMING {column} for {codes2names_dict}:\")\n",
    "        try:\n",
    "            tab_to_process = tab_to_process.with_columns(pl.col(column)).cast(pl.String)\n",
    "            # print(tab_to_process.head())\n",
    "            tab_to_process = tab_to_process.with_columns(\n",
    "                pl.Series(tab_to_process[column]).replace(codes2names_dict)\n",
    "            )\n",
    "            # print(\"AFTER RENAMING\", tab_to_process.head())\n",
    "        except Exception as e:\n",
    "            errors.append(\n",
    "                f\"Could not rename {column}. Columns found: {tab_to_process.columns}. Error: {e}\"\n",
    "            )\n",
    "            continue\n",
    "    return tab_to_process, errors\n",
    "\n",
    "db_path = os.path.join(save_location, \"db_mepram_sepsis_v2.sqlite3\")\n",
    "con = sqlite3.connect(db_path)\n",
    "tablenames = pl.read_database(\n",
    "    \"SELECT name FROM sqlite_master WHERE type = 'table'\", con\n",
    ")\n",
    "tablenames = sorted([x for x in tablenames.to_series()][:-1])\n",
    "tableshort = sorted([x.replace(\"tbl_\", \"\") for x in tablenames])\n",
    "print(tableshort)\n",
    "codes_tab = pl.read_database(\"SELECT * from tbl_codes2names\", con)\n",
    "codes_tab = codes_tab.drop(\"recode\")\n",
    "all_tabs = {}\n",
    "errors = {}\n",
    "for tabname, shortname in zip(tablenames, tableshort):\n",
    "    print(\"tabname:\", tabname, \"/// shortname:\", shortname)\n",
    "    tab_to_process = pl.read_database(\n",
    "        f\"SELECT * from {tabname}\", con, infer_schema_length=None\n",
    "    )\n",
    "    all_tabs[tabname], errors[tabname] = codes_to_names(\n",
    "        tab_to_process, codes_tab, shortname\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tabname, tab in all_tabs.items():\n",
    "    tab = pd.DataFrame(tab, columns=tab.columns)\n",
    "    exec(f\"{tabname} = eval('tab')\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DB Import\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "import re\n",
    "import os\n",
    "con = sqlite3.connect(os.path.join(save_location, \"db_mepram_sepsis_v3.sqlite3\"))\n",
    "\n",
    "cursor = con.cursor()\n",
    "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "\n",
    "with open(os.path.join(save_location, \"tbl_personid2center.sql\"), \"r\") as f:\n",
    "    sql_script = f.read()\n",
    "    cursor.executescript(sql_script)\n",
    "\n",
    "with open(os.path.join(save_location, \"tbl_microorganismos.sql\"), \"r\") as f:\n",
    "    sql_script = f.read()\n",
    "    cursor.executescript(sql_script)\n",
    "\n",
    "tables = [row[0] for row in cursor.fetchall()]\n",
    "\n",
    "print(tables)\n",
    "\n",
    "dataframes = {}\n",
    "for table in tables:\n",
    "    print(f\"Cargando la tabla: {table}\")\n",
    "    dataframes[table] = pd.read_sql_query(f\"SELECT * FROM {table}\", con)\n",
    "con.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "for name, table in dataframes.items():\n",
    "    print(name.replace(\"tbl\", \"formulario\"))\n",
    "    print(\"Columnas: \", re.sub(r\"[\\[\\]]\", \"\", str([x for x  in table.columns])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x,y in dataframes[\"tbl_infecciones_previas\"].groupby(\"person_id\"):\n",
    "    if len(y) > 4:\n",
    "        print(\"paciente: \", x)\n",
    "        df_to_show = y\n",
    "        break\n",
    "print(df_to_show)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to replace outliers by NaN using IQR and recode quantitative variables into ranges from 0 to 3.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_variables(df, variables):\n",
    "    \"\"\"\n",
    "    This function replaces outliers with NaN using IQR and recodes quantitative variables into ranges 0-3.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame containing the data.\n",
    "    variables (list): List of column names to process.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The DataFrame with outliers replaced by NaN and new recoded columns.\n",
    "    \"\"\"\n",
    "\n",
    "    df_processed = df.copy()\n",
    "    \n",
    "    for var in variables:\n",
    "        if var not in df.columns:\n",
    "            print(f\"Variable '{var}' not found in the DataFrame. It will be skipped.\")\n",
    "            continue\n",
    "\n",
    "        Q1 = df_processed[var].quantile(0.25)\n",
    "        Q3 = df_processed[var].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "\n",
    "        lower_limit = Q1 - 2 * IQR\n",
    "        upper_limit = Q3 + 2 * IQR\n",
    "\n",
    "        outliers_mask = (df_processed[var] < lower_limit) | (df_processed[var] > upper_limit)\n",
    "        outliers_count = outliers_mask.sum()\n",
    "        print(f\"{outliers_count} outliers replaced with NaN in the variable '{var}'.\")\n",
    "\n",
    "        df_processed.loc[outliers_mask, var] = np.nan\n",
    "\n",
    "        new_column = f\"{var}_recoded\"\n",
    "\n",
    "        df_processed[new_column] = pd.qcut(df_processed[var], \n",
    "                                           q=4, \n",
    "                                           labels=[0, 1, 2, 3], \n",
    "                                           duplicates='drop')\n",
    "\n",
    "        df_processed[new_column] = df_processed[new_column].astype('Int64') \n",
    "        \n",
    "        print(f\"Variable '{var}' recoded in the column '{new_column}'.\")\n",
    "    \n",
    "    return df_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tbl_paciente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pacientes = dataframes[\"tbl_paciente\"].merge(dataframes[\"tbl_personid2center\"], how=\"left\")\n",
    "\n",
    "df_pacientes[\"inf_previa_sino\"] = np.where(np.isin(df_pacientes[\"person_id\"].values, dataframes['tbl_infecciones_previas'][\"person_id\"].values) == True, 1, 0)\n",
    "\n",
    "bmr_previa = dataframes['tbl_infecciones_previas'].groupby(\"person_id\")[\"bmr_infec_previa\"].apply(lambda x: (x > 0).any())\n",
    "\n",
    "df_pacientes = df_pacientes.merge(bmr_previa, on=\"person_id\", how=\"left\").fillna(False)\n",
    "df_pacientes[\"bmr_infec_previa\"] = np.where(df_pacientes[\"bmr_infec_previa\"], 1, 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tbl_colonizaciones_previas\n",
    "\n",
    "No finalizado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tbl_comorbilidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tbl_comorbilidad = dataframes['tbl_comorbilidad']\n",
    "tbl_comorbilidad = pd.get_dummies(tbl_comorbilidad, columns=[\"tipo_cancer\",\"tipo_hepatopatia\"])\n",
    "print(tbl_comorbilidad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tbl_factores_riesgo_bmr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tbl_factores_riesgo_bmr = dataframes['tbl_factores_riesgo_bmr']\n",
    "print(tbl_factores_riesgo_bmr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tbl_sintomas\n",
    "\n",
    "1. **One-Hot Encoding con Duración**:\n",
    "   - Se crea una tabla pivote donde cada síntoma es una columna, y el valor corresponde a la suma de los días que el síntoma estuvo presente durante el ingreso.\n",
    "   - Los valores nulos se reemplazan por 0, indicando ausencia del síntoma.\n",
    "   - Finalmente los sintomas se recategorizan en: \n",
    "     - 0: Ausencia \n",
    "     - 1: Agudo (1-7 días)\n",
    "     - 2: Larga duración (>7 días)\n",
    "\n",
    "2. **Variables de Presencia/Ausencia**:\n",
    "   - A partir de la tabla de duración, se generan variables binarias que indican si el síntoma estuvo presente (1) o no (0).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tbl_sintomas = dataframes['tbl_sintomas']\n",
    "tbl_sintomas[\"sintoma\"] = tbl_sintomas[\"sintoma\"].astype(str)\n",
    "tbl_sintomas[\"sintoma\"] = \"sintoma_\" + tbl_sintomas[\"sintoma\"]\n",
    "tbl_sintomas_pivoted = tbl_sintomas.pivot_table(\n",
    "    index=[\"person_id\", \"fecha_ingreso_urgencias\"],  \n",
    "    columns=\"sintoma\",\n",
    "    values=\"duracion_sintoma\",\n",
    "    aggfunc=\"sum\",\n",
    "    fill_value=0).reset_index()\n",
    "\n",
    "presence_absence= tbl_sintomas_pivoted.copy()\n",
    "presence_absence.iloc[:, 2:] = (presence_absence.iloc[:, 2:] > 0).astype(int)\n",
    "\n",
    "recategorized= tbl_sintomas_pivoted.copy()\n",
    "recategorized.iloc[:, 2:] = recategorized.iloc[:, 2:].applymap(\n",
    "    lambda x:0 if x== 0 else (1 if x <= 7 else 2)\n",
    ")\n",
    "\n",
    "tbl_sintomas_complete = presence_absence.merge(\n",
    "    recategorized, on= ['person_id', 'fecha_ingreso_urgencias'], suffixes= (\"\", \"_categorico\")\n",
    ")\n",
    "\n",
    "print(tbl_sintomas_complete)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tbl_signos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tbl_signos = dataframes['tbl_signos']\n",
    "\n",
    "for col in tbl_signos.columns[2:]:\n",
    "    tbl_signos[col] = tbl_signos[col].apply(lambda x: re.findall(r'\\d+\\.\\d+|\\d+', str(x)))\n",
    "    tbl_signos[col] = tbl_signos[col].apply(lambda x: float(x[0]) if x else None)\n",
    "    \n",
    "tbl_signos['hipotermia_hipertermia'] = np.where(tbl_signos['temperatura'] >= 38, 2,\n",
    "                                np.where(tbl_signos['temperatura'] >= 36, 0, 1))\n",
    "tbl_signos['hipotermia_hipertermia'] = tbl_signos['hipotermia_hipertermia'].where(tbl_signos['temperatura'].notna())\n",
    "\n",
    "#tbl_signos = tbl_signos.drop(columns=['hipotermia_hipertermia'])\n",
    "tbl_signos['hipotension'] = np.where(tbl_signos['tension_arterial'].isna(), tbl_signos['hipotension'],\n",
    "                                     np.where(tbl_signos['tension_arterial'] <= 100, 1, 0))\n",
    "tbl_signos['taquipnea'] = np.where(tbl_signos['frec_respiratoria'].isna(), tbl_signos['taquipnea'],\n",
    "                                   np.where(tbl_signos['frec_respiratoria'] > 20, 1, 0))\n",
    "tbl_signos['taquicardia'] = np.where(tbl_signos['frec_cardiaca'].isna(), tbl_signos['taquicardia'],\n",
    "                                     np.where(tbl_signos['frec_cardiaca'] >90, 1, 0))\n",
    "tbl_signos['hipoxemia'] = np.where(tbl_signos['saturacion_o2'].isna(), tbl_signos['hipoxemia'],\n",
    "                                     np.where(tbl_signos['saturacion_o2'] >90, 0, 1))\n",
    "print(tbl_signos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove outliers and recode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vairables= ['temperatura', 'frec_respiratoria', 'frec_cardiaca', 'tension_arterial','saturacion_o2']\n",
    "tbl_signos = process_variables(tbl_signos, vairables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tbl_sepsis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tbl_sepsis = dataframes['tbl_sepsis']\n",
    "tbl_sepsis['lactato_serico'] = np.where(tbl_sepsis['lactato_serico'] == '<= 2 millimole per liter', 0, 1)\n",
    "for col in tbl_sepsis.columns[2:]:\n",
    "    tbl_sepsis[col] = tbl_sepsis[col].apply(lambda x: re.findall(r'\\d+\\.\\d+|\\d+', str(x)))\n",
    "    tbl_sepsis[col] = tbl_sepsis[col].apply(lambda x: float(x[0]) if len(x) > 0 else None)\n",
    "print(tbl_sepsis)\n",
    "\n",
    "# Remove outliers and recode\n",
    "\n",
    "variables =['proteina_c_reactiva']\n",
    "tbl_sepsis = procesar_variables(tbl_sepsis, variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tbl_infecciones_previas\n",
    "\n",
    "### Recodificación de microorganismos y dummy vars: \n",
    "Categorias restantes: \"ecoli\", \"virus\", \"fungi\", \"Staphylococcus aureus\", \"Pseudomonas aeruginosa\", \"Klebsiella pneumoniae\", \"Streptococcus pneumoniae\", \"Enterococcus\", \"Enterobacteria\", \"other bacteria\"\n",
    "\n",
    "### Calculo de eventos y tiempo medio entre eventos/paciente\n",
    "\n",
    "Se calcula el número de eventos por paciente (número de infecciones previas)\n",
    "Se calcula el tiempo entre eventos y en el caso de pacientes con >2 eventos, se calcula el tiempo medio. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load table with classification group for each organism in column label\n",
    "organism_classification = dataframes[\"tbl_microorganismos\"].copy()\n",
    "\n",
    "# Rename default values for final classification, use _ to separate target variables\n",
    "label_map = {\n",
    "    \"NOEB\": \"_Other bacteria\",\n",
    "    \"VIRUS\": \"_Virus\",\n",
    "    \"FUNGUS\": \"_Fungi\",\n",
    "    \"OEB\": \"_Enterobacteria\",\n",
    "    \"ECOLI\": \"Escherichia coli\",\n",
    "    \"SA\": \"Staphylococcus aureus\",\n",
    "    \"PSA\": \"Pseudomonas aeruginosa\",\n",
    "    \"KP\": \"Klebsiella pneumoniae\",\n",
    "    \"SP\": \"Streptococcus pneumoniae\",\n",
    "    \"EC\": \"Enterococcus\"\n",
    "}\n",
    "organism_classification[\"label\"] = organism_classification[\"label\"].map(label_map)\n",
    "# organism_classification[organism_classification.duplicated(subset=\"snomed_code\", keep=False)].sort_values(by=[\"snomed_code\", \"label\"])\n",
    "# There are snomed_codes with 2 labels, so always keep the most informative\n",
    "organism_classification = organism_classification.sort_values(by=[\"snomed_code\", \"label\"]).drop_duplicates(subset=\"snomed_code\", keep=\"first\")\n",
    "# Create a dictionary with {organism-snomed-code (value) : classification group (label)}\n",
    "organism_codes_map = dict(zip(organism_classification[\"snomed_code\"], organism_classification[\"label\"]))\n",
    "print(organism_codes_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tbl_infecciones_previas_mer = dataframes['tbl_infecciones_previas'].copy()\n",
    "\n",
    "\n",
    "# Pasamos a dummy los microorganismos y rellenamos la información con el sumatorio de cada dummy. De este modo no perdemos información.\n",
    "# Finalmente lo almacenamos en tbl_infecciones_previas_microorganismo\n",
    "tbl_infecciones_previas_mer['grupo_microorganismo'] = tbl_infecciones_previas_mer['microorganism_infec_prev'].astype(str).map(organism_codes_map)\n",
    "tbl_infecciones_previas_mer = tbl_infecciones_previas_mer.drop(columns=['microorganism_infec_prev'])\n",
    "tbl_infecciones_previas_mer[\"dummy\"] = 1\n",
    "tbl_infecciones_previas_mer = pd.get_dummies(tbl_infecciones_previas_mer, columns=['feno_resist_infec_prev'])\n",
    "tbl_infecciones_previas_microorganismos = tbl_infecciones_previas_mer.pivot_table(\n",
    "    index=[\"person_id\", \"fecha_ingreso_urgencias\"],  \n",
    "    columns=\"grupo_microorganismo\",\n",
    "    values=\"dummy\",\n",
    "    aggfunc=\"sum\",\n",
    "    fill_value=0).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tbl_infecciones_previas_microorganismos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculo del número de eventos por paciente y el tiempo medio entre cada visita. \n",
    "\n",
    "tbl_visitas_ip = tbl_infecciones_previas_mer.copy()\n",
    "fechas_invalidas = tbl_visitas_ip[~tbl_visitas_ip['fecha_infeccion'].str.match(r'\\d{4}-\\d{2}-\\d{2}')]\n",
    "tbl_visitas_ip.loc[tbl_visitas_ip['fecha_infeccion'] == '323-05-01', 'fecha_infeccion'] = '2023-05-01'\n",
    "tbl_visitas_ip['fecha_infeccion'] = pd.to_datetime(tbl_visitas_ip['fecha_infeccion'])\n",
    "tbl_visitas_ip = tbl_visitas_ip[['person_id', 'fecha_ingreso_urgencias', 'fecha_infeccion']]\n",
    "num_visitas = tbl_visitas_ip.groupby('person_id')['fecha_infeccion'].nunique().reset_index(name='numero_visitas')\n",
    "print(num_visitas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calcular tiempo de infeccion (dias) al ingreso. Tiempo medio en el caso de reinfecciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculo del tiempo entre las visitas en aquellos pacientes que tenga más de 1 visita\n",
    "\n",
    "tbl_visitas_ip = tbl_visitas_ip.sort_values(by=['person_id','fecha_infeccion'])\n",
    "num_dias = tbl_visitas_ip.copy()\n",
    "num_dias['numero_dias'] = num_dias.groupby('person_id')['fecha_infeccion'].diff().dt.days\n",
    "num_dias = num_dias[(num_dias['numero_dias'] > 0) & (num_dias['numero_dias'].notna())]\n",
    "num_dias = num_dias.groupby('person_id')['numero_dias'].mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculo de dias desde desde la última infección hasta el ingreso\n",
    "\n",
    "tbl_visitas_ip['fecha_ingreso_urgencias'] = pd.to_datetime(tbl_visitas_ip['fecha_ingreso_urgencias'])\n",
    "ultima_infeccion = tbl_visitas_ip.groupby('person_id')['fecha_infeccion'].max().reset_index(name= 'ultima_fecha')\n",
    "ultima_infeccion['ultima_fecha'] = pd.to_datetime(ultima_infeccion['ultima_fecha'])\n",
    "fecha_ingreso_unica = tbl_visitas_ip[['person_id', 'fecha_ingreso_urgencias']].drop_duplicates()\n",
    "ultima_infeccion_df = ultima_infeccion.merge(fecha_ingreso_unica, on='person_id')\n",
    "ultima_infeccion_df['tiempo_ultima'] = (ultima_infeccion_df['fecha_ingreso_urgencias'] - ultima_infeccion_df['ultima_fecha']).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge infecciones previas\n",
    "\n",
    "tbl_infecciones_complete = tbl_infecciones_previas_microorganismos.merge(num_visitas, on= ['person_id'], how= 'left')\n",
    "tbl_infecciones_complete = tbl_infecciones_complete.merge(num_dias, on= ['person_id'], how= 'left')\n",
    "tbl_infecciones_complete = tbl_infecciones_complete.merge(ultima_infeccion_df, on= ['person_id'], how= 'left')\n",
    "tbl_infecciones_complete = tbl_infecciones_complete.drop(columns=['fecha_ingreso_urgencias_y'])\n",
    "print(tbl_infecciones_complete)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge \n",
    "\n",
    "Combinación del dataset hasta este punto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = df_pacientes.merge(tbl_comorbilidad, on = ['person_id', 'fecha_ingreso_urgencias'], how= 'left')\n",
    "df_merged = df_merged.merge(tbl_factores_riesgo_bmr, on = ['person_id', 'fecha_ingreso_urgencias'], how= 'left')\n",
    "df_merged = df_merged.merge(tbl_sepsis, on= ['person_id', 'fecha_ingreso_urgencias'], how= 'left')\n",
    "df_merged = df_merged.merge(tbl_signos, on= ['person_id', 'fecha_ingreso_urgencias'], how= 'left')\n",
    "df_merged = df_merged.merge(tbl_sintomas_pivoted, on= ['person_id', 'fecha_ingreso_urgencias'], how= 'left')\n",
    "df_merged = df_merged.merge(tbl_infecciones_complete, on= ['person_id'], how= 'left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specifically process certain columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged[\"numero_visitas\"] = df_merged[\"numero_visitas\"].fillna(0)\n",
    "df_merged[\"mujer_gestante\"] = df_merged[\"mujer_gestante\"].replace(\"False\", 0.0).astype(int)\n",
    "# Process columns related with qsofa\n",
    "qsofa_components = [\"taquipnea\", \"estado_mental_alterado\", \"hipotension\"]\n",
    "df_merged[\"qsofa\"] = df_merged[\"qsofa\"].where(df_merged[qsofa_components].notna().all(axis=1), np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create html EDA report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile = ProfileReport(df_merged, title=\"MePRAM EDA report\")\n",
    "profile.to_notebook_iframe()\n",
    "profile.to_file(os.path.join(save_location, \"sintomas.html\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Missing Values Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def insert_linebreak(string, lengLabel=10):\n",
    "    return '\\n'.join(string[i:i+lengLabel] for i in range(0, len(string), lengLabel))\n",
    "merged_df = pd.read_csv(os.path.join(save_location, \"df_merged.csv\"))\n",
    "prev_num = 0\n",
    "\n",
    "def plot_missing_columns(subset_df, title=f\"Missing Data Matrix for columns 0 to 171\"):\n",
    "    missing_df = subset_df.isna()\n",
    "\n",
    "    missing_df.columns = subset_df.columns\n",
    "    missing_percent = missing_df.mean() * 100\n",
    "    columns_with_percent = [\n",
    "        f\"$\\\\bf{{{col}}}$ ({missing_percent[col]:.2f}%)\" if missing_percent[col] > 30 else f\"{col} ({missing_percent[col]:.2f}%)\"\n",
    "        for col in subset_df.columns\n",
    "    ]\n",
    "    plt.figure(figsize=(24, 6))\n",
    "    ax = sns.heatmap(missing_df, vmin=0, vmax=1, cbar=False,\n",
    "                xticklabels=columns_with_percent)\n",
    "    ax.tick_params(axis='x', which='minor', length=40)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Columns (% Missing)\")\n",
    "    plt.ylabel(\"Rows\", rotation=90)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_missing_columns(merged_df)\n",
    "\n",
    "missing_df = merged_df.isna()\n",
    "missing_df.columns = merged_df.columns\n",
    "missing_percent = missing_df.mean() * 100\n",
    "print([idx for idx,x in enumerate(missing_percent) if x > 30])\n",
    "dangerous_df = merged_df.iloc[:, [idx for idx,x in enumerate(missing_percent) if x > 30]]\n",
    "print(dangerous_df)\n",
    "plot_missing_columns(dangerous_df, f\"Missing Data Matrix of {len(dangerous_df.columns)} columns with > 30% NAs\")\n",
    "\n",
    "# Codigo para dividir por grupos de columnas\n",
    "\"\"\"for new_num in range(20,len(merged_df.columns) + 20, 20):\n",
    "    if new_num > len(merged_df.columns):\n",
    "        new_num = len(merged_df.columns)\n",
    "    subset_df = merged_df.iloc[:, prev_num:new_num]\n",
    "    plot_missing_columns(subset_df)\n",
    "    if new_num == len(merged_df.columns):\n",
    "        break\n",
    "    prev_num = new_num\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create first version of sintomn-signs-sepsis tab merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tbl_sepsis[[\"person_id\", \"fecha_ingreso_urgencias\", \"sepsis\", \"shock_septico\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sintom_sign_sepsis = tbl_signos.merge(tbl_sintomas_pivoted,  on=[\"person_id\", \"fecha_ingreso_urgencias\"], how=\"left\").merge(tbl_sepsis, on=[\"person_id\", \"fecha_ingreso_urgencias\"], how=\"left\")\n",
    "sintom_sign_sepsis = sintom_sign_sepsis.drop(\"sintoma_nan\", axis=1) #.fillna(value=0.0)\n",
    "#sintom_sign_sepsis[\"sintoma\"] = sintom_sign_sepsis[\"sintoma\"].str.replace(\"sintoma_\", \"\", regex=False).astype(\"float64\")\n",
    "print(sintom_sign_sepsis.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sintom_sign_sepsis.to_csv(os.path.join(save_location, \"df_sintom_signos_v1.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create first database version with all params except previous events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urgency_df = tbl_signos.merge(tbl_sintomas_pivoted,  on=[\"person_id\", \"fecha_ingreso_urgencias\"], how=\"left\")\n",
    "urgency_df = urgency_df.merge(df_pacientes, on=[\"person_id\", \"fecha_ingreso_urgencias\"], how=\"left\")\n",
    "urgency_df = urgency_df.merge(tbl_comorbilidad, on=[\"person_id\", \"fecha_ingreso_urgencias\"], how=\"left\")\n",
    "urgency_df = urgency_df.merge(tbl_sepsis, on=[\"person_id\", \"fecha_ingreso_urgencias\"], how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urgency_df.to_csv(os.path.join(save_location, \"df_sin_antecedentes_v1.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot = urgency_df.shape[0]\n",
    "for col in urgency_df.columns:\n",
    "    na_per = 1-len(urgency_df[col].dropna())/tot\n",
    "    if na_per > 0.15:\n",
    "        print(f\"Column {col} --> %NaN = {na_per}. Removed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLOT CORRELATION GRID (PAIRGRID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sintomas_columns = [col for col in sintom_sign_sepsis.columns if \"sintoma\" in col]\n",
    "binary_columns = [col for col in sintom_sign_sepsis.columns if sintom_sign_sepsis[col].dropna().value_counts().index.isin([0, 1]).all() and col not in sintomas_columns]\n",
    "numeric_columns = [col for col in sintom_sign_sepsis.columns if sintom_sign_sepsis[col].dtype in ['int64', 'float64'] and col not in binary_columns and col not in sintomas_columns]\n",
    "print(binary_columns)\n",
    "print(numeric_columns)\n",
    "print(sintomas_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "pg = sns.PairGrid(sintom_sign_sepsis[binary_columns], hue=\"sepsis\", palette=\"Set2\")\n",
    "pg.map_diag(sns.histplot)\n",
    "pg.map_offdiag(sns.barplot)\n",
    "pg.add_legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sintomas_columns_group1 = sintomas_columns[0:int(len(sintomas_columns)/2)]\n",
    "sintomas_columns_group1 = list(set(sintomas_columns_group1 + [\"sepsis\"]))\n",
    "sns.color_palette(\"tab10\")\n",
    "pg = sns.PairGrid(sintom_sign_sepsis[sintomas_columns_group1], hue=\"sepsis\", palette=\"Set2\")\n",
    "pg.map_diag(sns.histplot)\n",
    "pg.map_offdiag(sns.scatterplot, alpha=0.6)\n",
    "pg.add_legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sintomas_columns_group2 = sintomas_columns[int(len(sintomas_columns)/2):-1]\n",
    "sintomas_columns_group2 = list(set(sintomas_columns_group2 + [\"sepsis\"]))\n",
    "pg = sns.PairGrid(sintom_sign_sepsis[sintomas_columns_group2], hue=\"sepsis\", palette=\"Set2\")\n",
    "pg.map_diag(sns.histplot, alpha=0.5)\n",
    "pg.map_offdiag(sns.scatterplot, alpha=0.6)\n",
    "pg.add_legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns = list(set(numeric_columns + [\"sepsis\"]))\n",
    "pg = sns.PairGrid(sintom_sign_sepsis[numeric_columns].drop(\"person_id\", axis=1), hue=\"sepsis\", palette=\"Set2\")\n",
    "pg.map_diag(sns.kdeplot, alpha=0.8)\n",
    "pg.map_offdiag(sns.scatterplot, alpha=0.6)\n",
    "pg.add_legend()\n",
    "sns.color_palette(\"tab10\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
